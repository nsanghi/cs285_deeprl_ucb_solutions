{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUl_qfOR8JV6"
      },
      "source": [
        "##Setup\n",
        "\n",
        "You will need to make a copy of this notebook in your Google Drive before you can edit the homework files. You can do so with **File &rarr; Save a copy in Drive**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "iizPcHAp8LnA"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "from cs285.infrastructure.rl_trainer import RL_Trainer\n",
        "from cs285.agents.mb_agent import MBAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "g5xIOIpW8_jC"
      },
      "outputs": [],
      "source": [
        "#@title set up virtual display\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "# For later\n",
        "from cs285.infrastructure.local_utils_video import (\n",
        "    wrap_env,\n",
        "    show_video\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "2rsWAWaK9BVp"
      },
      "outputs": [],
      "source": [
        "#@title test virtual display\n",
        "\n",
        "#@markdown If you see a video of a four-legged ant fumbling about, setup is complete!\n",
        "\n",
        "import gym\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "env = wrap_env(gym.make(\"Ant-v2\"))\n",
        "\n",
        "observation = env.reset()\n",
        "for i in range(10):\n",
        "    env.render(mode='rgb_array')\n",
        "    obs, rew, term, _ = env.step(env.action_space.sample() ) \n",
        "    if term:\n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "print('Loading video...')\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QizpiHDh9Fwk"
      },
      "source": [
        "## Editing Code\n",
        "\n",
        "To edit code, click the folder icon on the left menu. Navigate to the corresponding file (`cs285_f2021/...`). Double click a file to open an editor. There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window). We sync your edits to Google Drive so that you won't lose your work in the event of an instance timeout, but you will need to re-mount your Google Drive and re-install packages with every new instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nii6qk2C9Ipk"
      },
      "source": [
        "## Run MBRL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4t7FUeEG9Dkf"
      },
      "outputs": [],
      "source": [
        "#@title imports\n",
        "import os\n",
        "import time\n",
        "\n",
        "from cs285.infrastructure.rl_trainer import RL_Trainer\n",
        "from cs285.agents.mb_agent import MBAgent\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "2fXlzARJ9i-t"
      },
      "outputs": [],
      "source": [
        "#@title runtime arguments\n",
        "\n",
        "class Args:\n",
        "\n",
        "  def __getitem__(self, key):\n",
        "    return getattr(self, key)\n",
        "\n",
        "  def __setitem__(self, key, val):\n",
        "    setattr(self, key, val)\n",
        "\n",
        "  def __contains__(self, key):\n",
        "    return hasattr(self, key)\n",
        "\n",
        "  env_name = \"cheetah-cs285-v0\" #@param [\"cheetah-cs285-v0\", \"obstacles-cs285-v0\", \"reacher-cs285-v0\"]\n",
        "  exp_name = \"TODO\"#@param\n",
        "  n_iter = 20 #@param {type:\"integer\"}\n",
        "\n",
        "  if env_name == 'reacher-cs285-v0':\n",
        "    ep_len = 200\n",
        "  if env_name == 'cheetah-cs285-v0':\n",
        "    ep_len = 500\n",
        "  if env_name == 'obstacles-cs285-v0':\n",
        "    ep_len = 100\n",
        "\n",
        "  #@markdown batches and steps\n",
        "  batch_size = 8000 #@param {type: \"integer\"}\n",
        "  eval_batch_size = 400 #@param {type: \"integer\"}\n",
        "  train_batch_size = 512 #@param {type: \"integer\"}\n",
        "  batch_size_initial = 20000 #@param {type: \"integer\"}\n",
        "\n",
        "  num_agent_train_steps_per_iter = 1000 #@param {type: \"integer\"}\n",
        "\n",
        "  #@markdown MBRL parameters\n",
        "  ensemble_size = 3 #@param {type:\"integer\"}\n",
        "  mpc_horizon = 10 #@param {type:\"integer\"}\n",
        "  mpc_num_action_sequences = 1000 #@param {type:\"integer\"}\n",
        "  mpc_action_sampling_strategy = 'random' #@param [\"random\", \"cem\"]\n",
        "  cem_iterations = 4 #@param {type: \"integer\"}\n",
        "  cem_num_elites = 5 #@param {type: \"integer\"}\n",
        "  cem_alpha = 1.0 #@param {type: \"raw\"}\n",
        "\n",
        "  #@markdown Learning parameters\n",
        "  learning_rate = 0.001 #@param {type:\"raw\"}\n",
        "  n_layers = 2 #@param {type:\"integer\"}\n",
        "  size = 250 #@param {type:\"integer\"}\n",
        "  add_sl_noise = True #@param {type:\"boolean\"}\n",
        "\n",
        "  #@markdown system\n",
        "  save_params = False #@param {type: \"boolean\"}\n",
        "  no_gpu = False #@param {type: \"boolean\"}\n",
        "  which_gpu = 0 #@param {type: \"integer\"}\n",
        "  seed = 1 #@param {type: \"integer\"}\n",
        "\n",
        "  #@markdown logging\n",
        "  ## default is to not log video so\n",
        "  ## that logs are small enough to be\n",
        "  ## uploaded to gradscope\n",
        "  video_log_freq = -1 #@param {type: \"integer\"}\n",
        "  scalar_log_freq = 1#@param {type: \"integer\"}\n",
        "\n",
        "\n",
        "args = Args()\n",
        "\n",
        "## ensure compatibility with hw1 code\n",
        "args['train_batch_size'] = args['batch_size']\n",
        "\n",
        "if args['video_log_freq'] > 0:\n",
        "  import warnings\n",
        "  warnings.warn(\n",
        "      '''\\nLogging videos will make eventfiles too'''\n",
        "      '''\\nlarge for the autograder. Set video_log_freq = -1'''\n",
        "      '''\\nfor the runs you intend to submit.''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0cJlp6s-ogO"
      },
      "outputs": [],
      "source": [
        "#@title create directories for logging\n",
        "\n",
        "# data_path = '../../data/'\n",
        "\n",
        "# if not (os.path.exists(data_path)):\n",
        "#     os.makedirs(data_path)\n",
        "\n",
        "# logdir = 'hw4_' + args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "# logdir = os.path.join(data_path, logdir)\n",
        "# args['logdir'] = logdir\n",
        "# if not(os.path.exists(logdir)):\n",
        "#     os.makedirs(logdir)\n",
        "\n",
        "# print(\"LOGGING TO: \", logdir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I525KFRN-42s"
      },
      "outputs": [],
      "source": [
        "#@title Define Model Based trainer\n",
        "\n",
        "class MB_Trainer(object):\n",
        "\n",
        "    def __init__(self, params):\n",
        "\n",
        "        computation_graph_args = {\n",
        "            'ensemble_size': params['ensemble_size'],\n",
        "            'n_layers': params['n_layers'],\n",
        "            'size': params['size'],\n",
        "            'learning_rate': params['learning_rate'],\n",
        "            }\n",
        "\n",
        "        train_args = {\n",
        "            'num_agent_train_steps_per_iter': params['num_agent_train_steps_per_iter'],\n",
        "        }\n",
        "\n",
        "        controller_args = {\n",
        "            'mpc_horizon': params['mpc_horizon'],\n",
        "            'mpc_num_action_sequences': params['mpc_num_action_sequences'],\n",
        "            'mpc_action_sampling_strategy': params['mpc_action_sampling_strategy'],\n",
        "            'cem_iterations': params['cem_iterations'],\n",
        "            'cem_num_elites': params['cem_num_elites'],\n",
        "            'cem_alpha': params['cem_alpha'],\n",
        "        }\n",
        "\n",
        "        agent_params = {**computation_graph_args, **train_args, **controller_args}\n",
        "\n",
        "        self.params = params\n",
        "        self.params['agent_class'] = MBAgent\n",
        "        self.params['agent_params'] = agent_params\n",
        "\n",
        "        self.rl_trainer = RL_Trainer(self.params)\n",
        "\n",
        "    def run_training_loop(self):\n",
        "\n",
        "        self.rl_trainer.run_training_loop(\n",
        "            self.params['n_iter'],\n",
        "            collect_policy = self.rl_trainer.agent.actor,\n",
        "            eval_policy = self.rl_trainer.agent.actor,\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wF4LSRGn-_Cv"
      },
      "outputs": [],
      "source": [
        "#@title run training\n",
        "\n",
        "# trainer = MB_Trainer(args)\n",
        "# trainer.run_training_loop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Q2\n",
        "#python cs285/scripts/run_hw4_mb.py --exp_name q2_obstacles_singleiteration \n",
        "# --env_name obstacles-cs285-v0 --add_sl_noise --num_agent_train_steps_per_iter 20 --n_iter 1 \n",
        "# --batch_size_initial 5000 --batch_size 1000 --mpc_horizon 10 --mpc_action_sampling_strategy 'random'\n",
        "\n",
        "args = Args()\n",
        "args.exp_name = 'q2_obstacles_singleiteration'\n",
        "args.env_name = 'obstacles-cs285-v0'\n",
        "args.add_sl_noise = True\n",
        "args.num_agent_train_steps_per_iter = 20\n",
        "args.n_iter = 1\n",
        "args.batch_size_initial = 5000\n",
        "args.batch_size = 1000\n",
        "args.mpc_horizon = 10\n",
        "args.mpc_action_sampling_strategy = 'random'\n",
        "\n",
        "data_path = '../../data/'\n",
        "logdir = 'hw4_' + args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join(data_path, logdir)\n",
        "args['logdir'] = logdir\n",
        "if not(os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)\n",
        "\n",
        "print(\"LOGGING TO: \", logdir)\n",
        "\n",
        "trainer = MB_Trainer(args)\n",
        "trainer.run_training_loop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Problem 3\n",
        "# python cs285/scripts/run_hw4_mb.py --exp_name q3_obstacles --env_name obstacles-cs285-v0 \n",
        "# --add_sl_noise --num_agent_train_steps_per_iter 20 --batch_size_initial 5000 --batch_size 1000 \n",
        "# --mpc_horizon 10 --n_iter 12 --mpc_action_sampling_strategy 'random'\n",
        "args = Args()\n",
        "args.exp_name = 'q3_obstacles'\n",
        "args.env_name = 'obstacles-cs285-v0'\n",
        "args.add_sl_noise = True\n",
        "args.num_agent_train_steps_per_iter = 20\n",
        "args.batch_size_initial = 5000\n",
        "args.batch_size = 1000\n",
        "args.mpc_horizon = 10\n",
        "args.n_iter = 12\n",
        "args.mpc_action_sampling_strategy = 'random'\n",
        "\n",
        "data_path = '../../data/'\n",
        "logdir = 'hw4_' + args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join(data_path, logdir)\n",
        "args['logdir'] = logdir\n",
        "if not(os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)\n",
        "\n",
        "print(\"LOGGING TO: \", logdir)\n",
        "\n",
        "trainer = MB_Trainer(args)\n",
        "trainer.run_training_loop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Q3 2nd command\n",
        "# python cs285/scripts/run_hw4_mb.py --exp_name q3_reacher \n",
        "# --env_name reacher-cs285-v0 --add_sl_noise --mpc_horizon 10 \n",
        "# --num_agent_train_steps_per_iter 1000 --batch_size_initial 5000 \n",
        "# --batch_size 5000 --n_iter 15 --mpc_action_sampling_strategy 'random\n",
        "\n",
        "args = Args()\n",
        "args.exp_name = 'q3_reacher'\n",
        "args.env_name = 'reacher-cs285-v0'\n",
        "args.add_sl_noise = True\n",
        "args.mpc_horizon = 10\n",
        "args.num_agent_train_steps_per_iter = 1000\n",
        "args.batch_size_initial = 5000\n",
        "args.batch_size = 5000\n",
        "args.n_iter = 15\n",
        "args.mpc_action_sampling_strategy = 'random'\n",
        "\n",
        "data_path = '../../data/'\n",
        "logdir = 'hw4_' + args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join(data_path, logdir)\n",
        "args['logdir'] = logdir\n",
        "if not(os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)\n",
        "\n",
        "print(\"LOGGING TO: \", logdir)\n",
        "\n",
        "trainer = MB_Trainer(args)\n",
        "trainer.run_training_loop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Q# - 3rd command\n",
        "# python cs285/scripts/run_hw4_mb.py --exp_name q3_cheetah --env_name cheetah-cs285-v0 \n",
        "# --mpc_horizon 15 --add_sl_noise --num_agent_train_steps_per_iter 1500 \n",
        "# --batch_size_initial 5000 --batch_size 5000 --n_iter 20 \n",
        "# --mpc_action_sampling_strategy 'random\n",
        "\n",
        "args = Args()\n",
        "args.exp_name = 'q3_cheetah'\n",
        "args.env_name = 'cheetah-cs285-v0'\n",
        "args.mpc_horizon = 15\n",
        "args.add_sl_noise = True\n",
        "args.num_agent_train_steps_per_iter = 1500\n",
        "args.batch_size_initial = 5000\n",
        "args.batch_size = 5000\n",
        "args.n_iter = 20\n",
        "args.mpc_action_sampling_strategy = 'random'\n",
        "\n",
        "data_path = '../../data/'\n",
        "logdir = 'hw4_' + args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join(data_path, logdir)\n",
        "args['logdir'] = logdir\n",
        "if not(os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)\n",
        "\n",
        "print(\"LOGGING TO: \", logdir)\n",
        "\n",
        "trainer = MB_Trainer(args)\n",
        "trainer.run_training_loop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -388.1238708496094\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -388.1238708496094\n",
            "Eval_MinReturn : -388.1238708496094\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -416.55743408203125\n",
            "Train_StdReturn : 11.799728393554688\n",
            "Train_MaxReturn : -404.7576904296875\n",
            "Train_MinReturn : -428.3571472167969\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 32000\n",
            "TimeSinceStart : 1013.988255739212\n",
            "Training Loss : 0.16945350170135498\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 13 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -483.6348876953125\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -483.6348876953125\n",
            "Eval_MinReturn : -483.6348876953125\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -396.0587463378906\n",
            "Train_StdReturn : 23.027862548828125\n",
            "Train_MaxReturn : -373.0308837890625\n",
            "Train_MinReturn : -419.08660888671875\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 33000\n",
            "TimeSinceStart : 1095.177721977234\n",
            "Training Loss : 0.16431814432144165\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 14 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -394.8548278808594\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -394.8548278808594\n",
            "Eval_MinReturn : -394.8548278808594\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -405.350341796875\n",
            "Train_StdReturn : 31.632278442382812\n",
            "Train_MaxReturn : -373.71807861328125\n",
            "Train_MinReturn : -436.9826354980469\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 34000\n",
            "TimeSinceStart : 1176.880383491516\n",
            "Training Loss : 0.1590091586112976\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "LOGGING TO:  ../../data/hw4_q4_reacher_numseq100_reacher-cs285-v0_23-10-2021_08-24-03\n",
            "########################\n",
            "logging outputs to  ../../data/hw4_q4_reacher_numseq100_reacher-cs285-v0_23-10-2021_08-24-03\n",
            "########################\n",
            "Using GPU id 0\n",
            "Using action sampling strategy: random\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -469.1702880859375\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -469.1702880859375\n",
            "Eval_MinReturn : -469.1702880859375\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -4489.67822265625\n",
            "Train_StdReturn : 1366.086669921875\n",
            "Train_MaxReturn : -2048.99365234375\n",
            "Train_MinReturn : -6259.1376953125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 20000\n",
            "TimeSinceStart : 13.534255981445312\n",
            "Training Loss : 0.1951325535774231\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -387.33740234375\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -387.33740234375\n",
            "Eval_MinReturn : -387.33740234375\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -433.179443359375\n",
            "Train_StdReturn : 21.834060668945312\n",
            "Train_MaxReturn : -411.3453674316406\n",
            "Train_MinReturn : -455.01348876953125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 21000\n",
            "TimeSinceStart : 35.675599813461304\n",
            "Training Loss : 0.18059243261814117\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -416.3486022949219\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -416.3486022949219\n",
            "Eval_MinReturn : -416.3486022949219\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -400.8190612792969\n",
            "Train_StdReturn : 39.67657470703125\n",
            "Train_MaxReturn : -361.1424865722656\n",
            "Train_MinReturn : -440.4956359863281\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 22000\n",
            "TimeSinceStart : 57.808992862701416\n",
            "Training Loss : 0.1759827584028244\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -406.8572692871094\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -406.8572692871094\n",
            "Eval_MinReturn : -406.8572692871094\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -407.604248046875\n",
            "Train_StdReturn : 29.231124877929688\n",
            "Train_MaxReturn : -378.3731384277344\n",
            "Train_MinReturn : -436.83538818359375\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 23000\n",
            "TimeSinceStart : 80.08887267112732\n",
            "Training Loss : 0.1782914251089096\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -359.78192138671875\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -359.78192138671875\n",
            "Eval_MinReturn : -359.78192138671875\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -393.3475036621094\n",
            "Train_StdReturn : 47.7315673828125\n",
            "Train_MaxReturn : -345.6159362792969\n",
            "Train_MinReturn : -441.0790710449219\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 24000\n",
            "TimeSinceStart : 102.43101787567139\n",
            "Training Loss : 0.17547370493412018\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 5 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -343.0430603027344\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -343.0430603027344\n",
            "Eval_MinReturn : -343.0430603027344\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -389.2342529296875\n",
            "Train_StdReturn : 35.63874816894531\n",
            "Train_MaxReturn : -353.5954895019531\n",
            "Train_MinReturn : -424.87298583984375\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 25000\n",
            "TimeSinceStart : 124.71932291984558\n",
            "Training Loss : 0.17550231516361237\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 6 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -417.17352294921875\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -417.17352294921875\n",
            "Eval_MinReturn : -417.17352294921875\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -364.09600830078125\n",
            "Train_StdReturn : 14.000503540039062\n",
            "Train_MaxReturn : -350.0954895019531\n",
            "Train_MinReturn : -378.09649658203125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 26000\n",
            "TimeSinceStart : 146.9877393245697\n",
            "Training Loss : 0.1667395681142807\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 7 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -332.19061279296875\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -332.19061279296875\n",
            "Eval_MinReturn : -332.19061279296875\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -341.23052978515625\n",
            "Train_StdReturn : 22.492767333984375\n",
            "Train_MaxReturn : -318.7377624511719\n",
            "Train_MinReturn : -363.7232971191406\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 27000\n",
            "TimeSinceStart : 169.31190943717957\n",
            "Training Loss : 0.1661340594291687\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 8 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -390.558837890625\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -390.558837890625\n",
            "Eval_MinReturn : -390.558837890625\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -341.5766296386719\n",
            "Train_StdReturn : 14.28485107421875\n",
            "Train_MaxReturn : -327.2917785644531\n",
            "Train_MinReturn : -355.8614807128906\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 28000\n",
            "TimeSinceStart : 191.8625144958496\n",
            "Training Loss : 0.1703558713197708\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 9 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -308.0379333496094\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -308.0379333496094\n",
            "Eval_MinReturn : -308.0379333496094\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -368.822509765625\n",
            "Train_StdReturn : 21.958206176757812\n",
            "Train_MaxReturn : -346.8642883300781\n",
            "Train_MinReturn : -390.78070068359375\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 29000\n",
            "TimeSinceStart : 214.47710394859314\n",
            "Training Loss : 0.16793054342269897\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 10 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -345.0750427246094\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -345.0750427246094\n",
            "Eval_MinReturn : -345.0750427246094\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -385.50177001953125\n",
            "Train_StdReturn : 41.24909973144531\n",
            "Train_MaxReturn : -344.252685546875\n",
            "Train_MinReturn : -426.7508850097656\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 30000\n",
            "TimeSinceStart : 236.9194619655609\n",
            "Training Loss : 0.16343335807323456\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 11 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -316.8644104003906\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -316.8644104003906\n",
            "Eval_MinReturn : -316.8644104003906\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -392.897705078125\n",
            "Train_StdReturn : 75.24809265136719\n",
            "Train_MaxReturn : -317.6496276855469\n",
            "Train_MinReturn : -468.14581298828125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 31000\n",
            "TimeSinceStart : 259.1221537590027\n",
            "Training Loss : 0.16402919590473175\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 12 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -390.1135559082031\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -390.1135559082031\n",
            "Eval_MinReturn : -390.1135559082031\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -355.0127258300781\n",
            "Train_StdReturn : 23.85272216796875\n",
            "Train_MaxReturn : -331.1600036621094\n",
            "Train_MinReturn : -378.8654479980469\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 32000\n",
            "TimeSinceStart : 281.33734250068665\n",
            "Training Loss : 0.16421090066432953\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 13 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -316.3648986816406\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -316.3648986816406\n",
            "Eval_MinReturn : -316.3648986816406\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -380.12109375\n",
            "Train_StdReturn : 66.10467529296875\n",
            "Train_MaxReturn : -314.01641845703125\n",
            "Train_MinReturn : -446.22576904296875\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 33000\n",
            "TimeSinceStart : 303.7713797092438\n",
            "Training Loss : 0.16469329595565796\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 14 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -311.37298583984375\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -311.37298583984375\n",
            "Eval_MinReturn : -311.37298583984375\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -333.2133483886719\n",
            "Train_StdReturn : 13.970428466796875\n",
            "Train_MaxReturn : -319.242919921875\n",
            "Train_MinReturn : -347.18377685546875\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 34000\n",
            "TimeSinceStart : 326.0171127319336\n",
            "Training Loss : 0.15796732902526855\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "LOGGING TO:  ../../data/hw4_q4_reacher_numseq1000_reacher-cs285-v0_23-10-2021_08-29-29\n",
            "########################\n",
            "logging outputs to  ../../data/hw4_q4_reacher_numseq1000_reacher-cs285-v0_23-10-2021_08-29-29\n",
            "########################\n",
            "Using GPU id 0\n",
            "Using action sampling strategy: random\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -442.9765625\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -442.9765625\n",
            "Eval_MinReturn : -442.9765625\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -4489.67822265625\n",
            "Train_StdReturn : 1366.086669921875\n",
            "Train_MaxReturn : -2048.99365234375\n",
            "Train_MinReturn : -6259.1376953125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 20000\n",
            "TimeSinceStart : 15.746174573898315\n",
            "Training Loss : 0.1951325535774231\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -316.2596740722656\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -316.2596740722656\n",
            "Eval_MinReturn : -316.2596740722656\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -390.0329895019531\n",
            "Train_StdReturn : 15.235626220703125\n",
            "Train_MaxReturn : -374.79736328125\n",
            "Train_MinReturn : -405.26861572265625\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 21000\n",
            "TimeSinceStart : 44.43871831893921\n",
            "Training Loss : 0.17822067439556122\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -304.52618408203125\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -304.52618408203125\n",
            "Eval_MinReturn : -304.52618408203125\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -301.375\n",
            "Train_StdReturn : 9.724105834960938\n",
            "Train_MaxReturn : -291.6509094238281\n",
            "Train_MinReturn : -311.09912109375\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 22000\n",
            "TimeSinceStart : 73.08853483200073\n",
            "Training Loss : 0.18343184888362885\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -316.1692199707031\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -316.1692199707031\n",
            "Eval_MinReturn : -316.1692199707031\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -339.75946044921875\n",
            "Train_StdReturn : 1.6173858642578125\n",
            "Train_MaxReturn : -338.1420593261719\n",
            "Train_MinReturn : -341.3768310546875\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 23000\n",
            "TimeSinceStart : 101.98989391326904\n",
            "Training Loss : 0.1777372807264328\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -297.4039611816406\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -297.4039611816406\n",
            "Eval_MinReturn : -297.4039611816406\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -296.08111572265625\n",
            "Train_StdReturn : 16.09417724609375\n",
            "Train_MaxReturn : -279.9869384765625\n",
            "Train_MinReturn : -312.17529296875\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 24000\n",
            "TimeSinceStart : 130.80900955200195\n",
            "Training Loss : 0.17610995471477509\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 5 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -269.1882629394531\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -269.1882629394531\n",
            "Eval_MinReturn : -269.1882629394531\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -315.4718933105469\n",
            "Train_StdReturn : 1.821044921875\n",
            "Train_MaxReturn : -313.6508483886719\n",
            "Train_MinReturn : -317.2929382324219\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 25000\n",
            "TimeSinceStart : 159.7127149105072\n",
            "Training Loss : 0.16557195782661438\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 6 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -321.6070861816406\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -321.6070861816406\n",
            "Eval_MinReturn : -321.6070861816406\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -313.644775390625\n",
            "Train_StdReturn : 4.05902099609375\n",
            "Train_MaxReturn : -309.58575439453125\n",
            "Train_MinReturn : -317.70379638671875\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 26000\n",
            "TimeSinceStart : 188.39033007621765\n",
            "Training Loss : 0.16986866295337677\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 7 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -271.3118896484375\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -271.3118896484375\n",
            "Eval_MinReturn : -271.3118896484375\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -308.09429931640625\n",
            "Train_StdReturn : 17.716629028320312\n",
            "Train_MaxReturn : -290.3776550292969\n",
            "Train_MinReturn : -325.8109130859375\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 27000\n",
            "TimeSinceStart : 217.23240184783936\n",
            "Training Loss : 0.16775096952915192\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 8 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -283.5929260253906\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -283.5929260253906\n",
            "Eval_MinReturn : -283.5929260253906\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -307.1703796386719\n",
            "Train_StdReturn : 1.004119873046875\n",
            "Train_MaxReturn : -306.166259765625\n",
            "Train_MinReturn : -308.17449951171875\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 28000\n",
            "TimeSinceStart : 246.37458753585815\n",
            "Training Loss : 0.17766173183918\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 9 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -293.4083251953125\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -293.4083251953125\n",
            "Eval_MinReturn : -293.4083251953125\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -304.35858154296875\n",
            "Train_StdReturn : 8.366058349609375\n",
            "Train_MaxReturn : -295.9925231933594\n",
            "Train_MinReturn : -312.7246398925781\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 29000\n",
            "TimeSinceStart : 275.28985714912415\n",
            "Training Loss : 0.1664646565914154\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 10 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -261.17620849609375\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -261.17620849609375\n",
            "Eval_MinReturn : -261.17620849609375\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -290.7095642089844\n",
            "Train_StdReturn : 6.587615966796875\n",
            "Train_MaxReturn : -284.1219482421875\n",
            "Train_MinReturn : -297.29718017578125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 30000\n",
            "TimeSinceStart : 304.0800633430481\n",
            "Training Loss : 0.1645938605070114\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 11 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -286.0639343261719\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -286.0639343261719\n",
            "Eval_MinReturn : -286.0639343261719\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -282.12677001953125\n",
            "Train_StdReturn : 18.865371704101562\n",
            "Train_MaxReturn : -263.2613830566406\n",
            "Train_MinReturn : -300.99212646484375\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 31000\n",
            "TimeSinceStart : 332.92081665992737\n",
            "Training Loss : 0.1593060940504074\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 12 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -359.0994567871094\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -359.0994567871094\n",
            "Eval_MinReturn : -359.0994567871094\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -299.0159912109375\n",
            "Train_StdReturn : 0.7753753662109375\n",
            "Train_MaxReturn : -298.2406311035156\n",
            "Train_MinReturn : -299.7913818359375\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 32000\n",
            "TimeSinceStart : 361.9688892364502\n",
            "Training Loss : 0.16398076713085175\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 13 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -302.6969299316406\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -302.6969299316406\n",
            "Eval_MinReturn : -302.6969299316406\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -299.0576171875\n",
            "Train_StdReturn : 34.576873779296875\n",
            "Train_MaxReturn : -264.4807434082031\n",
            "Train_MinReturn : -333.6344909667969\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 33000\n",
            "TimeSinceStart : 390.99065828323364\n",
            "Training Loss : 0.16277818381786346\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 14 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -290.2713928222656\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -290.2713928222656\n",
            "Eval_MinReturn : -290.2713928222656\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -297.52911376953125\n",
            "Train_StdReturn : 4.3089141845703125\n",
            "Train_MaxReturn : -293.2201843261719\n",
            "Train_MinReturn : -301.8380126953125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 34000\n",
            "TimeSinceStart : 419.9209921360016\n",
            "Training Loss : 0.1560664176940918\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Q4\n",
        "data_path = '../../data/'\n",
        "\n",
        "# python cs285/scripts/run_hw4_mb.py --exp_name q4_reacher_horizon5 --env_name reacher-cs285-v0 \n",
        "# --add_sl_noise --mpc_horizon 5 --mpc_action_sampling_strategy 'random' \n",
        "# --num_agent_train_steps_per_iter 1000 --batch_size 800 --n_iter 15 \n",
        "# --mpc_action_sampling_strategy 'random'\n",
        "args = Args()\n",
        "args.exp_name = 'q4_reacher_horizon5'\n",
        "args.env_name = 'reacher-cs285-v0'\n",
        "args.add_sl_noise = True\n",
        "args.mpc_horizon = 5\n",
        "args.num_agent_train_steps_per_iter = 1000\n",
        "args.batch_size = 800\n",
        "args.n_iter = 15\n",
        "args.mpc_action_sampling_strategy = 'random'\n",
        "\n",
        "logdir = 'hw4_' + args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join(data_path, logdir)\n",
        "args['logdir'] = logdir\n",
        "if not(os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)\n",
        "\n",
        "print(\"LOGGING TO: \", logdir)\n",
        "\n",
        "trainer = MB_Trainer(args)\n",
        "trainer.run_training_loop()\n",
        "\n",
        "# \n",
        "# python cs285/scripts/run_hw4_mb.py --exp_name q4_reacher_horizon15 --env_name reacher-cs285-v0 \n",
        "# --add_sl_noise --mpc_horizon 15 --num_agent_train_steps_per_iter 1000 --batch_size 800 \n",
        "# --n_iter 15 --mpc_action_sampling_strategy 'random'\n",
        "\n",
        "args = Args()\n",
        "args.exp_name = 'q4_reacher_horizon15'\n",
        "args.env_name = 'reacher-cs285-v0'\n",
        "args.add_sl_noise = True\n",
        "args.mpc_horizon = 15\n",
        "args.num_agent_train_steps_per_iter = 1000\n",
        "args.batch_size = 800\n",
        "args.n_iter = 15\n",
        "args.mpc_action_sampling_strategy = 'random'\n",
        "\n",
        "logdir = 'hw4_' + args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join(data_path, logdir)\n",
        "args['logdir'] = logdir\n",
        "if not(os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)\n",
        "\n",
        "print(\"LOGGING TO: \", logdir)\n",
        "\n",
        "trainer = MB_Trainer(args)\n",
        "trainer.run_training_loop()\n",
        "\n",
        "# python cs285/scripts/run_hw4_mb.py --exp_name q4_reacher_horizon30 --env_name reacher-cs285-v0 \n",
        "# --add_sl_noise --mpc_horizon 30 --num_agent_train_steps_per_iter 1000 --batch_size 800 \n",
        "# --n_iter 15 --mpc_action_sampling_strategy 'random'\n",
        "# \n",
        "args = Args()\n",
        "args.exp_name = 'q4_reacher_horizon30'\n",
        "args.env_name = 'reacher-cs285-v0'\n",
        "args.add_sl_noise = True\n",
        "args.mpc_horizon = 30\n",
        "args.num_agent_train_steps_per_iter = 1000\n",
        "args.batch_size = 800\n",
        "args.n_iter = 15\n",
        "args.mpc_action_sampling_strategy = 'random'\n",
        "\n",
        "logdir = 'hw4_' + args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join(data_path, logdir)\n",
        "args['logdir'] = logdir\n",
        "if not(os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)\n",
        "\n",
        "print(\"LOGGING TO: \", logdir)\n",
        "\n",
        "trainer = MB_Trainer(args)\n",
        "trainer.run_training_loop()\n",
        "\n",
        "\n",
        "\n",
        "# python cs285/scripts/run_hw4_mb.py --exp_name q4_reacher_numseq100 --env_name reacher-cs285-v0 \n",
        "# --add_sl_noise --mpc_horizon 10 --num_agent_train_steps_per_iter 1000 --batch_size 800 \n",
        "# --n_iter 15 --mpc_num_action_sequences 100 --mpc_action_sampling_strategy 'random'\n",
        "# \n",
        "\n",
        "args = Args()\n",
        "args.exp_name = 'q4_reacher_numseq100'\n",
        "args.env_name = 'reacher-cs285-v0'\n",
        "args.add_sl_noise = True\n",
        "args.mpc_horizon = 10\n",
        "args.num_agent_train_steps_per_iter = 1000\n",
        "args.batch_size = 800\n",
        "args.n_iter = 15\n",
        "args.mpc_num_action_sequences = 100\n",
        "args.mpc_action_sampling_strategy = 'random'\n",
        "\n",
        "logdir = 'hw4_' + args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join(data_path, logdir)\n",
        "args['logdir'] = logdir\n",
        "if not(os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)\n",
        "\n",
        "print(\"LOGGING TO: \", logdir)\n",
        "\n",
        "trainer = MB_Trainer(args)\n",
        "trainer.run_training_loop()\n",
        "\n",
        "# python cs285/scripts/run_hw4_mb.py --exp_name q4_reacher_numseq1000 --env_name reacher-cs285-v0 \n",
        "# --add_sl_noise --mpc_horizon 10 --num_agent_train_steps_per_iter 1000 --batch_size 800 \n",
        "# --n_iter 15 --mpc_num_action_sequences 1000 --mpc_action_sampling_strategy 'random'\n",
        "# \n",
        "args = Args()\n",
        "args.exp_name = 'q4_reacher_numseq1000'\n",
        "args.env_name = 'reacher-cs285-v0'\n",
        "args.add_sl_noise = True\n",
        "args.mpc_horizon = 10\n",
        "args.num_agent_train_steps_per_iter = 1000\n",
        "args.batch_size = 800\n",
        "args.n_iter = 15\n",
        "args.mpc_num_action_sequences = 1000\n",
        "args.mpc_action_sampling_strategy = 'random'\n",
        "\n",
        "logdir = 'hw4_' + args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join(data_path, logdir)\n",
        "args['logdir'] = logdir\n",
        "if not(os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)\n",
        "\n",
        "print(\"LOGGING TO: \", logdir)\n",
        "\n",
        "trainer = MB_Trainer(args)\n",
        "trainer.run_training_loop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOGGING TO:  ../../data/hw4_q4_reacher_ensemble1_reacher-cs285-v0_23-10-2021_08-45-43\n",
            "########################\n",
            "logging outputs to  ../../data/hw4_q4_reacher_ensemble1_reacher-cs285-v0_23-10-2021_08-45-43\n",
            "########################\n",
            "Using GPU id 0\n",
            "Using action sampling strategy: random\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -2279.636962890625\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -2279.636962890625\n",
            "Eval_MinReturn : -2279.636962890625\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -4489.67822265625\n",
            "Train_StdReturn : 1366.086669921875\n",
            "Train_MaxReturn : -2048.99365234375\n",
            "Train_MinReturn : -6259.1376953125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 20000\n",
            "TimeSinceStart : 7.638526916503906\n",
            "Training Loss : 0.18329642713069916\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -299.31121826171875\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -299.31121826171875\n",
            "Eval_MinReturn : -299.31121826171875\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -704.0867919921875\n",
            "Train_StdReturn : 241.37435913085938\n",
            "Train_MaxReturn : -462.71240234375\n",
            "Train_MinReturn : -945.4611206054688\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 21000\n",
            "TimeSinceStart : 18.477601766586304\n",
            "Training Loss : 0.185460165143013\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -298.4017333984375\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -298.4017333984375\n",
            "Eval_MinReturn : -298.4017333984375\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -311.268310546875\n",
            "Train_StdReturn : 28.13189697265625\n",
            "Train_MaxReturn : -283.13641357421875\n",
            "Train_MinReturn : -339.40020751953125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 22000\n",
            "TimeSinceStart : 29.366503477096558\n",
            "Training Loss : 0.18387262523174286\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -331.7930908203125\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -331.7930908203125\n",
            "Eval_MinReturn : -331.7930908203125\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -364.9019775390625\n",
            "Train_StdReturn : 38.85264587402344\n",
            "Train_MaxReturn : -326.04931640625\n",
            "Train_MinReturn : -403.7546081542969\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 23000\n",
            "TimeSinceStart : 40.262452602386475\n",
            "Training Loss : 0.1660369634628296\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -295.64190673828125\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -295.64190673828125\n",
            "Eval_MinReturn : -295.64190673828125\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -398.250732421875\n",
            "Train_StdReturn : 0.8787689208984375\n",
            "Train_MaxReturn : -397.3719787597656\n",
            "Train_MinReturn : -399.1295166015625\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 24000\n",
            "TimeSinceStart : 51.13122868537903\n",
            "Training Loss : 0.17879639565944672\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 5 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -748.9515380859375\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -748.9515380859375\n",
            "Eval_MinReturn : -748.9515380859375\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -301.57855224609375\n",
            "Train_StdReturn : 12.319549560546875\n",
            "Train_MaxReturn : -289.2590026855469\n",
            "Train_MinReturn : -313.8981018066406\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 25000\n",
            "TimeSinceStart : 61.95926761627197\n",
            "Training Loss : 0.17110076546669006\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 6 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -306.154541015625\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -306.154541015625\n",
            "Eval_MinReturn : -306.154541015625\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -299.3983459472656\n",
            "Train_StdReturn : 23.3531494140625\n",
            "Train_MaxReturn : -276.0451965332031\n",
            "Train_MinReturn : -322.7514953613281\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 26000\n",
            "TimeSinceStart : 72.79561066627502\n",
            "Training Loss : 0.17544303834438324\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 7 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -365.89617919921875\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -365.89617919921875\n",
            "Eval_MinReturn : -365.89617919921875\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -367.85797119140625\n",
            "Train_StdReturn : 48.771881103515625\n",
            "Train_MaxReturn : -319.0860900878906\n",
            "Train_MinReturn : -416.6298522949219\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 27000\n",
            "TimeSinceStart : 83.70107674598694\n",
            "Training Loss : 0.17647044360637665\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 8 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -316.0667419433594\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -316.0667419433594\n",
            "Eval_MinReturn : -316.0667419433594\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -406.07440185546875\n",
            "Train_StdReturn : 6.774749755859375\n",
            "Train_MaxReturn : -399.2996520996094\n",
            "Train_MinReturn : -412.8491516113281\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 28000\n",
            "TimeSinceStart : 94.66308403015137\n",
            "Training Loss : 0.17468030750751495\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 9 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -305.8189392089844\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -305.8189392089844\n",
            "Eval_MinReturn : -305.8189392089844\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -328.93572998046875\n",
            "Train_StdReturn : 32.08378601074219\n",
            "Train_MaxReturn : -296.8519592285156\n",
            "Train_MinReturn : -361.01953125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 29000\n",
            "TimeSinceStart : 105.6370701789856\n",
            "Training Loss : 0.17591576278209686\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 10 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -306.40692138671875\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -306.40692138671875\n",
            "Eval_MinReturn : -306.40692138671875\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -273.621826171875\n",
            "Train_StdReturn : 8.841949462890625\n",
            "Train_MaxReturn : -264.7798767089844\n",
            "Train_MinReturn : -282.4637756347656\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 30000\n",
            "TimeSinceStart : 116.65086102485657\n",
            "Training Loss : 0.1625397503376007\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 11 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -306.9350280761719\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -306.9350280761719\n",
            "Eval_MinReturn : -306.9350280761719\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -296.8699645996094\n",
            "Train_StdReturn : 16.708526611328125\n",
            "Train_MaxReturn : -280.16143798828125\n",
            "Train_MinReturn : -313.5784912109375\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 31000\n",
            "TimeSinceStart : 127.55780577659607\n",
            "Training Loss : 0.16333140432834625\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 12 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -316.1452941894531\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -316.1452941894531\n",
            "Eval_MinReturn : -316.1452941894531\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -308.45184326171875\n",
            "Train_StdReturn : 46.43989562988281\n",
            "Train_MaxReturn : -262.0119323730469\n",
            "Train_MinReturn : -354.8917236328125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 32000\n",
            "TimeSinceStart : 138.51224184036255\n",
            "Training Loss : 0.16644196212291718\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 13 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -329.72796630859375\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -329.72796630859375\n",
            "Eval_MinReturn : -329.72796630859375\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -383.9583435058594\n",
            "Train_StdReturn : 56.50537109375\n",
            "Train_MaxReturn : -327.4529724121094\n",
            "Train_MinReturn : -440.4637145996094\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 33000\n",
            "TimeSinceStart : 149.41875505447388\n",
            "Training Loss : 0.1608562469482422\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 14 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -300.58111572265625\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -300.58111572265625\n",
            "Eval_MinReturn : -300.58111572265625\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -327.2574768066406\n",
            "Train_StdReturn : 35.329254150390625\n",
            "Train_MaxReturn : -291.92822265625\n",
            "Train_MinReturn : -362.58673095703125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 34000\n",
            "TimeSinceStart : 160.33117747306824\n",
            "Training Loss : 0.1589815467596054\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "LOGGING TO:  ../../data/hw4_q4_reacher_ensemble3_reacher-cs285-v0_23-10-2021_08-48-23\n",
            "########################\n",
            "logging outputs to  ../../data/hw4_q4_reacher_ensemble3_reacher-cs285-v0_23-10-2021_08-48-23\n",
            "########################\n",
            "Using GPU id 0\n",
            "Using action sampling strategy: random\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -442.9765625\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -442.9765625\n",
            "Eval_MinReturn : -442.9765625\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -4489.67822265625\n",
            "Train_StdReturn : 1366.086669921875\n",
            "Train_MaxReturn : -2048.99365234375\n",
            "Train_MinReturn : -6259.1376953125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 20000\n",
            "TimeSinceStart : 15.629091501235962\n",
            "Training Loss : 0.1951325535774231\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -316.2596740722656\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -316.2596740722656\n",
            "Eval_MinReturn : -316.2596740722656\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -390.0329895019531\n",
            "Train_StdReturn : 15.235626220703125\n",
            "Train_MaxReturn : -374.79736328125\n",
            "Train_MinReturn : -405.26861572265625\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 21000\n",
            "TimeSinceStart : 44.47661471366882\n",
            "Training Loss : 0.17822067439556122\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -304.52618408203125\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -304.52618408203125\n",
            "Eval_MinReturn : -304.52618408203125\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -301.375\n",
            "Train_StdReturn : 9.724105834960938\n",
            "Train_MaxReturn : -291.6509094238281\n",
            "Train_MinReturn : -311.09912109375\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 22000\n",
            "TimeSinceStart : 73.41872262954712\n",
            "Training Loss : 0.18343184888362885\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -316.1692199707031\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -316.1692199707031\n",
            "Eval_MinReturn : -316.1692199707031\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -339.75946044921875\n",
            "Train_StdReturn : 1.6173858642578125\n",
            "Train_MaxReturn : -338.1420593261719\n",
            "Train_MinReturn : -341.3768310546875\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 23000\n",
            "TimeSinceStart : 102.0826301574707\n",
            "Training Loss : 0.1777372807264328\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -297.4039611816406\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -297.4039611816406\n",
            "Eval_MinReturn : -297.4039611816406\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -296.08111572265625\n",
            "Train_StdReturn : 16.09417724609375\n",
            "Train_MaxReturn : -279.9869384765625\n",
            "Train_MinReturn : -312.17529296875\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 24000\n",
            "TimeSinceStart : 131.03622317314148\n",
            "Training Loss : 0.17610995471477509\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 5 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -269.1882629394531\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -269.1882629394531\n",
            "Eval_MinReturn : -269.1882629394531\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -315.4718933105469\n",
            "Train_StdReturn : 1.821044921875\n",
            "Train_MaxReturn : -313.6508483886719\n",
            "Train_MinReturn : -317.2929382324219\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 25000\n",
            "TimeSinceStart : 159.8943862915039\n",
            "Training Loss : 0.16557195782661438\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 6 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -321.6070861816406\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -321.6070861816406\n",
            "Eval_MinReturn : -321.6070861816406\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -313.644775390625\n",
            "Train_StdReturn : 4.05902099609375\n",
            "Train_MaxReturn : -309.58575439453125\n",
            "Train_MinReturn : -317.70379638671875\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 26000\n",
            "TimeSinceStart : 188.77200174331665\n",
            "Training Loss : 0.16986866295337677\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 7 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -271.3118896484375\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -271.3118896484375\n",
            "Eval_MinReturn : -271.3118896484375\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -308.09429931640625\n",
            "Train_StdReturn : 17.716629028320312\n",
            "Train_MaxReturn : -290.3776550292969\n",
            "Train_MinReturn : -325.8109130859375\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 27000\n",
            "TimeSinceStart : 217.7011046409607\n",
            "Training Loss : 0.16775096952915192\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 8 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -283.5929260253906\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -283.5929260253906\n",
            "Eval_MinReturn : -283.5929260253906\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -307.1703796386719\n",
            "Train_StdReturn : 1.004119873046875\n",
            "Train_MaxReturn : -306.166259765625\n",
            "Train_MinReturn : -308.17449951171875\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 28000\n",
            "TimeSinceStart : 246.644948720932\n",
            "Training Loss : 0.17766173183918\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 9 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -293.4083251953125\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -293.4083251953125\n",
            "Eval_MinReturn : -293.4083251953125\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -304.35858154296875\n",
            "Train_StdReturn : 8.366058349609375\n",
            "Train_MaxReturn : -295.9925231933594\n",
            "Train_MinReturn : -312.7246398925781\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 29000\n",
            "TimeSinceStart : 275.37211418151855\n",
            "Training Loss : 0.1664646565914154\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 10 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -261.17620849609375\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -261.17620849609375\n",
            "Eval_MinReturn : -261.17620849609375\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -290.7095642089844\n",
            "Train_StdReturn : 6.587615966796875\n",
            "Train_MaxReturn : -284.1219482421875\n",
            "Train_MinReturn : -297.29718017578125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 30000\n",
            "TimeSinceStart : 304.15684747695923\n",
            "Training Loss : 0.1645938605070114\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 11 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -286.0639343261719\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -286.0639343261719\n",
            "Eval_MinReturn : -286.0639343261719\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -282.12677001953125\n",
            "Train_StdReturn : 18.865371704101562\n",
            "Train_MaxReturn : -263.2613830566406\n",
            "Train_MinReturn : -300.99212646484375\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 31000\n",
            "TimeSinceStart : 332.9787471294403\n",
            "Training Loss : 0.1593060940504074\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 12 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -359.0994567871094\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -359.0994567871094\n",
            "Eval_MinReturn : -359.0994567871094\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -299.0159912109375\n",
            "Train_StdReturn : 0.7753753662109375\n",
            "Train_MaxReturn : -298.2406311035156\n",
            "Train_MinReturn : -299.7913818359375\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 32000\n",
            "TimeSinceStart : 362.0312399864197\n",
            "Training Loss : 0.16398076713085175\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 13 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -302.6969299316406\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -302.6969299316406\n",
            "Eval_MinReturn : -302.6969299316406\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -299.0576171875\n",
            "Train_StdReturn : 34.576873779296875\n",
            "Train_MaxReturn : -264.4807434082031\n",
            "Train_MinReturn : -333.6344909667969\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 33000\n",
            "TimeSinceStart : 391.1126358509064\n",
            "Training Loss : 0.16277818381786346\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 14 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -290.2713928222656\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -290.2713928222656\n",
            "Eval_MinReturn : -290.2713928222656\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -297.52911376953125\n",
            "Train_StdReturn : 4.3089141845703125\n",
            "Train_MaxReturn : -293.2201843261719\n",
            "Train_MinReturn : -301.8380126953125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 34000\n",
            "TimeSinceStart : 419.90250754356384\n",
            "Training Loss : 0.1560664176940918\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "LOGGING TO:  ../../data/hw4_q4_reacher_ensemble5_reacher-cs285-v0_23-10-2021_08-55-23\n",
            "########################\n",
            "logging outputs to  ../../data/hw4_q4_reacher_ensemble5_reacher-cs285-v0_23-10-2021_08-55-23\n",
            "########################\n",
            "Using GPU id 0\n",
            "Using action sampling strategy: random\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -425.89996337890625\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -425.89996337890625\n",
            "Eval_MinReturn : -425.89996337890625\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -4489.67822265625\n",
            "Train_StdReturn : 1366.086669921875\n",
            "Train_MaxReturn : -2048.99365234375\n",
            "Train_MinReturn : -6259.1376953125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 20000\n",
            "TimeSinceStart : 24.191649436950684\n",
            "Training Loss : 0.19329619407653809\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -349.82568359375\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -349.82568359375\n",
            "Eval_MinReturn : -349.82568359375\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -378.81793212890625\n",
            "Train_StdReturn : 28.454696655273438\n",
            "Train_MaxReturn : -350.3632507324219\n",
            "Train_MinReturn : -407.27264404296875\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 21000\n",
            "TimeSinceStart : 71.47948241233826\n",
            "Training Loss : 0.18553626537322998\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -275.13922119140625\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -275.13922119140625\n",
            "Eval_MinReturn : -275.13922119140625\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -329.006103515625\n",
            "Train_StdReturn : 14.257293701171875\n",
            "Train_MaxReturn : -314.7488098144531\n",
            "Train_MinReturn : -343.2633972167969\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 22000\n",
            "TimeSinceStart : 118.9044725894928\n",
            "Training Loss : 0.17711026966571808\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -273.423583984375\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -273.423583984375\n",
            "Eval_MinReturn : -273.423583984375\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -280.959716796875\n",
            "Train_StdReturn : 2.4240875244140625\n",
            "Train_MaxReturn : -278.5356140136719\n",
            "Train_MinReturn : -283.3837890625\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 23000\n",
            "TimeSinceStart : 165.79478979110718\n",
            "Training Loss : 0.17629209160804749\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -270.75848388671875\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -270.75848388671875\n",
            "Eval_MinReturn : -270.75848388671875\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -284.69598388671875\n",
            "Train_StdReturn : 9.810134887695312\n",
            "Train_MaxReturn : -274.8858642578125\n",
            "Train_MinReturn : -294.5061340332031\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 24000\n",
            "TimeSinceStart : 212.72451186180115\n",
            "Training Loss : 0.17404481768608093\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 5 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -294.95025634765625\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -294.95025634765625\n",
            "Eval_MinReturn : -294.95025634765625\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -274.8666076660156\n",
            "Train_StdReturn : 3.035400390625\n",
            "Train_MaxReturn : -271.8312072753906\n",
            "Train_MinReturn : -277.9020080566406\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 25000\n",
            "TimeSinceStart : 259.6714849472046\n",
            "Training Loss : 0.17095832526683807\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 6 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -293.498779296875\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -293.498779296875\n",
            "Eval_MinReturn : -293.498779296875\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -296.99066162109375\n",
            "Train_StdReturn : 7.5190887451171875\n",
            "Train_MaxReturn : -289.4715881347656\n",
            "Train_MinReturn : -304.509765625\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 26000\n",
            "TimeSinceStart : 307.0329806804657\n",
            "Training Loss : 0.17653818428516388\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 7 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -297.3445739746094\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -297.3445739746094\n",
            "Eval_MinReturn : -297.3445739746094\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -290.2373352050781\n",
            "Train_StdReturn : 4.055938720703125\n",
            "Train_MaxReturn : -286.181396484375\n",
            "Train_MinReturn : -294.29327392578125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 27000\n",
            "TimeSinceStart : 354.3278338909149\n",
            "Training Loss : 0.16790412366390228\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 8 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -283.00677490234375\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -283.00677490234375\n",
            "Eval_MinReturn : -283.00677490234375\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -273.30706787109375\n",
            "Train_StdReturn : 1.6163330078125\n",
            "Train_MaxReturn : -271.69073486328125\n",
            "Train_MinReturn : -274.92340087890625\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 28000\n",
            "TimeSinceStart : 401.67654728889465\n",
            "Training Loss : 0.1713307946920395\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 9 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -292.22650146484375\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -292.22650146484375\n",
            "Eval_MinReturn : -292.22650146484375\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -297.5145263671875\n",
            "Train_StdReturn : 11.077377319335938\n",
            "Train_MaxReturn : -286.4371643066406\n",
            "Train_MinReturn : -308.5919189453125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 29000\n",
            "TimeSinceStart : 448.80906653404236\n",
            "Training Loss : 0.17207291722297668\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 10 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -256.1447448730469\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -256.1447448730469\n",
            "Eval_MinReturn : -256.1447448730469\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -302.8898620605469\n",
            "Train_StdReturn : 11.18023681640625\n",
            "Train_MaxReturn : -291.7096252441406\n",
            "Train_MinReturn : -314.0700988769531\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 30000\n",
            "TimeSinceStart : 495.8290557861328\n",
            "Training Loss : 0.16645701229572296\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 11 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -328.8890380859375\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -328.8890380859375\n",
            "Eval_MinReturn : -328.8890380859375\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -293.73199462890625\n",
            "Train_StdReturn : 1.0905609130859375\n",
            "Train_MaxReturn : -292.6414489746094\n",
            "Train_MinReturn : -294.82257080078125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 31000\n",
            "TimeSinceStart : 542.8387227058411\n",
            "Training Loss : 0.16627487540245056\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 12 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -254.19468688964844\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -254.19468688964844\n",
            "Eval_MinReturn : -254.19468688964844\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -280.3876037597656\n",
            "Train_StdReturn : 16.821258544921875\n",
            "Train_MaxReturn : -263.56634521484375\n",
            "Train_MinReturn : -297.2088623046875\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 32000\n",
            "TimeSinceStart : 590.1314759254456\n",
            "Training Loss : 0.16429759562015533\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 13 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -270.4608154296875\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -270.4608154296875\n",
            "Eval_MinReturn : -270.4608154296875\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -270.32562255859375\n",
            "Train_StdReturn : 0.5598907470703125\n",
            "Train_MaxReturn : -269.7657165527344\n",
            "Train_MinReturn : -270.885498046875\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 33000\n",
            "TimeSinceStart : 637.0975227355957\n",
            "Training Loss : 0.1628120392560959\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 14 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -269.51708984375\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : -269.51708984375\n",
            "Eval_MinReturn : -269.51708984375\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -291.933349609375\n",
            "Train_StdReturn : 6.02276611328125\n",
            "Train_MaxReturn : -285.91058349609375\n",
            "Train_MinReturn : -297.95611572265625\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 34000\n",
            "TimeSinceStart : 684.2102236747742\n",
            "Training Loss : 0.16462913155555725\n",
            "Initial_DataCollection_AverageReturn : -4489.67822265625\n",
            "Done logging...\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# python cs285/scripts/run_hw4_mb.py --exp_name q4_reacher_ensemble1 --env_name reacher-cs285-v0 \n",
        "# --ensemble_size 1 --add_sl_noise --mpc_horizon 10 --num_agent_train_steps_per_iter 1000 \n",
        "# --batch_size 800 --n_iter 15 --mpc_action_sampling_strategy 'random'\n",
        "# \n",
        "args = Args()\n",
        "args.exp_name = 'q4_reacher_ensemble1'\n",
        "args.env_name = 'reacher-cs285-v0'\n",
        "args.ensemble_size = 1\n",
        "args.add_sl_noise = True\n",
        "args.mpc_horizon = 10\n",
        "args.num_agent_train_steps_per_iter = 1000\n",
        "args.batch_size = 800\n",
        "args.n_iter = 15\n",
        "args.mpc_action_sampling_strategy = 'random'\n",
        "\n",
        "logdir = 'hw4_' + args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join(data_path, logdir)\n",
        "args['logdir'] = logdir\n",
        "if not(os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)\n",
        "\n",
        "print(\"LOGGING TO: \", logdir)\n",
        "\n",
        "trainer = MB_Trainer(args)\n",
        "trainer.run_training_loop()\n",
        "\n",
        "# python cs285/scripts/run_hw4_mb.py --exp_name q4_reacher_ensemble3 --env_name reacher-cs285-v0 \n",
        "# --ensemble_size 3 --add_sl_noise --mpc_horizon 10 --num_agent_train_steps_per_iter 1000 \n",
        "# --batch_size800 --n_iter 15 --mpc_action_sampling_strategy 'random'\n",
        "# \n",
        "args = Args()\n",
        "args.exp_name = 'q4_reacher_ensemble3'\n",
        "args.env_name = 'reacher-cs285-v0'\n",
        "args.ensemble_size = 3\n",
        "args.add_sl_noise = True\n",
        "args.mpc_horizon = 10\n",
        "args.num_agent_train_steps_per_iter = 1000\n",
        "args.batch_size = 800\n",
        "args.n_iter = 15\n",
        "args.mpc_action_sampling_strategy = 'random'\n",
        "\n",
        "logdir = 'hw4_' + args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join(data_path, logdir)\n",
        "args['logdir'] = logdir\n",
        "if not(os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)\n",
        "\n",
        "print(\"LOGGING TO: \", logdir)\n",
        "\n",
        "trainer = MB_Trainer(args)\n",
        "trainer.run_training_loop()\n",
        "\n",
        "# python cs285/scripts/run_hw4_mb.py --exp_name q4_reacher_ensemble5 --env_name reacher-cs285-v0 \n",
        "# --ensemble_size 5 --add_sl_noise --mpc_horizon 10 --num_agent_train_steps_per_iter 1000 --batch_size800 --n_iter 15 \n",
        "# --mpc_action_sampling_strategy 'random'\n",
        "\n",
        "args = Args()\n",
        "args.exp_name = 'q4_reacher_ensemble5'\n",
        "args.env_name = 'reacher-cs285-v0'\n",
        "args.ensemble_size = 5\n",
        "args.add_sl_noise = True\n",
        "args.mpc_horizon = 10\n",
        "args.num_agent_train_steps_per_iter = 1000\n",
        "args.batch_size = 800\n",
        "args.n_iter = 15\n",
        "args.mpc_action_sampling_strategy = 'random'\n",
        "\n",
        "logdir = 'hw4_' + args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join(data_path, logdir)\n",
        "args['logdir'] = logdir\n",
        "if not(os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)\n",
        "\n",
        "print(\"LOGGING TO: \", logdir)\n",
        "\n",
        "trainer = MB_Trainer(args)\n",
        "trainer.run_training_loop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOGGING TO:  ../../data/hw4_q5_cheetah_random_cheetah-cs285-v0_23-10-2021_09-06-48\n",
            "########################\n",
            "logging outputs to  ../../data/hw4_q5_cheetah_random_cheetah-cs285-v0_23-10-2021_09-06-48\n",
            "########################\n",
            "Using GPU id 0\n",
            "Using action sampling strategy: random\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 186.33712768554688\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 186.33712768554688\n",
            "Eval_MinReturn : 186.33712768554688\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -2504.96630859375\n",
            "Train_StdReturn : 321.12841796875\n",
            "Train_MaxReturn : -2021.8564453125\n",
            "Train_MinReturn : -2978.619140625\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 5000\n",
            "TimeSinceStart : 20.848231315612793\n",
            "Training Loss : 0.07709154486656189\n",
            "Initial_DataCollection_AverageReturn : -2504.96630859375\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 276.9820251464844\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 276.9820251464844\n",
            "Eval_MinReturn : 276.9820251464844\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : 139.25375366210938\n",
            "Train_StdReturn : 21.237056732177734\n",
            "Train_MaxReturn : 167.9618377685547\n",
            "Train_MinReturn : 109.93438720703125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 10000\n",
            "TimeSinceStart : 157.51125955581665\n",
            "Training Loss : 0.09605660289525986\n",
            "Initial_DataCollection_AverageReturn : -2504.96630859375\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 189.16421508789062\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 189.16421508789062\n",
            "Eval_MinReturn : 189.16421508789062\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : 225.76705932617188\n",
            "Train_StdReturn : 40.17702102661133\n",
            "Train_MaxReturn : 289.82635498046875\n",
            "Train_MinReturn : 160.8535919189453\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 15000\n",
            "TimeSinceStart : 295.8141906261444\n",
            "Training Loss : 0.10052883625030518\n",
            "Initial_DataCollection_AverageReturn : -2504.96630859375\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 353.5780029296875\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 353.5780029296875\n",
            "Eval_MinReturn : 353.5780029296875\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : 261.56549072265625\n",
            "Train_StdReturn : 36.64844512939453\n",
            "Train_MaxReturn : 320.2608947753906\n",
            "Train_MinReturn : 192.08297729492188\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 20000\n",
            "TimeSinceStart : 434.08102345466614\n",
            "Training Loss : 0.10086335986852646\n",
            "Initial_DataCollection_AverageReturn : -2504.96630859375\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 270.9199523925781\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 270.9199523925781\n",
            "Eval_MinReturn : 270.9199523925781\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : 290.55084228515625\n",
            "Train_StdReturn : 32.508949279785156\n",
            "Train_MaxReturn : 338.9970703125\n",
            "Train_MinReturn : 239.37030029296875\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 25000\n",
            "TimeSinceStart : 572.1887879371643\n",
            "Training Loss : 0.1008116602897644\n",
            "Initial_DataCollection_AverageReturn : -2504.96630859375\n",
            "Done logging...\n",
            "\n",
            "\n",
            "LOGGING TO:  ../../data/hw4_q5_cheetah_cem_2_cheetah-cs285-v0_23-10-2021_09-16-20\n",
            "########################\n",
            "logging outputs to  ../../data/hw4_q5_cheetah_cem_2_cheetah-cs285-v0_23-10-2021_09-16-20\n",
            "########################\n",
            "Using GPU id 0\n",
            "Using action sampling strategy: cem\n",
            "CEM params: alpha=1.0, num_elites=5, iterations=2\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 186.33712768554688\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 186.33712768554688\n",
            "Eval_MinReturn : 186.33712768554688\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -2504.96630859375\n",
            "Train_StdReturn : 321.12841796875\n",
            "Train_MaxReturn : -2021.8564453125\n",
            "Train_MinReturn : -2978.619140625\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 5000\n",
            "TimeSinceStart : 20.67042827606201\n",
            "Training Loss : 0.07709154486656189\n",
            "Initial_DataCollection_AverageReturn : -2504.96630859375\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 276.9820251464844\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 276.9820251464844\n",
            "Eval_MinReturn : 276.9820251464844\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : 139.25375366210938\n",
            "Train_StdReturn : 21.237056732177734\n",
            "Train_MaxReturn : 167.9618377685547\n",
            "Train_MinReturn : 109.93438720703125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 10000\n",
            "TimeSinceStart : 158.67124581336975\n",
            "Training Loss : 0.09605660289525986\n",
            "Initial_DataCollection_AverageReturn : -2504.96630859375\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 189.16421508789062\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 189.16421508789062\n",
            "Eval_MinReturn : 189.16421508789062\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : 225.76705932617188\n",
            "Train_StdReturn : 40.17702102661133\n",
            "Train_MaxReturn : 289.82635498046875\n",
            "Train_MinReturn : 160.8535919189453\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 15000\n",
            "TimeSinceStart : 295.7261960506439\n",
            "Training Loss : 0.10052883625030518\n",
            "Initial_DataCollection_AverageReturn : -2504.96630859375\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 353.5780029296875\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 353.5780029296875\n",
            "Eval_MinReturn : 353.5780029296875\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : 261.56549072265625\n",
            "Train_StdReturn : 36.64844512939453\n",
            "Train_MaxReturn : 320.2608947753906\n",
            "Train_MinReturn : 192.08297729492188\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 20000\n",
            "TimeSinceStart : 432.85197949409485\n",
            "Training Loss : 0.10086335986852646\n",
            "Initial_DataCollection_AverageReturn : -2504.96630859375\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 270.9199523925781\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 270.9199523925781\n",
            "Eval_MinReturn : 270.9199523925781\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : 290.55084228515625\n",
            "Train_StdReturn : 32.508949279785156\n",
            "Train_MaxReturn : 338.9970703125\n",
            "Train_MinReturn : 239.37030029296875\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 25000\n",
            "TimeSinceStart : 571.3002023696899\n",
            "Training Loss : 0.1008116602897644\n",
            "Initial_DataCollection_AverageReturn : -2504.96630859375\n",
            "Done logging...\n",
            "\n",
            "\n",
            "LOGGING TO:  ../../data/hw4_q5_cheetah_cem_4_cheetah-cs285-v0_23-10-2021_09-25-51\n",
            "########################\n",
            "logging outputs to  ../../data/hw4_q5_cheetah_cem_4_cheetah-cs285-v0_23-10-2021_09-25-51\n",
            "########################\n",
            "Using GPU id 0\n",
            "Using action sampling strategy: cem\n",
            "CEM params: alpha=1.0, num_elites=5, iterations=4\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 186.33712768554688\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 186.33712768554688\n",
            "Eval_MinReturn : 186.33712768554688\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : -2504.96630859375\n",
            "Train_StdReturn : 321.12841796875\n",
            "Train_MaxReturn : -2021.8564453125\n",
            "Train_MinReturn : -2978.619140625\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 5000\n",
            "TimeSinceStart : 20.88016986846924\n",
            "Training Loss : 0.07709154486656189\n",
            "Initial_DataCollection_AverageReturn : -2504.96630859375\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 276.9820251464844\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 276.9820251464844\n",
            "Eval_MinReturn : 276.9820251464844\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : 139.25375366210938\n",
            "Train_StdReturn : 21.237056732177734\n",
            "Train_MaxReturn : 167.9618377685547\n",
            "Train_MinReturn : 109.93438720703125\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 10000\n",
            "TimeSinceStart : 159.81047654151917\n",
            "Training Loss : 0.09605660289525986\n",
            "Initial_DataCollection_AverageReturn : -2504.96630859375\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 189.16421508789062\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 189.16421508789062\n",
            "Eval_MinReturn : 189.16421508789062\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : 225.76705932617188\n",
            "Train_StdReturn : 40.17702102661133\n",
            "Train_MaxReturn : 289.82635498046875\n",
            "Train_MinReturn : 160.8535919189453\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 15000\n",
            "TimeSinceStart : 298.08719396591187\n",
            "Training Loss : 0.10052883625030518\n",
            "Initial_DataCollection_AverageReturn : -2504.96630859375\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 353.5780029296875\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 353.5780029296875\n",
            "Eval_MinReturn : 353.5780029296875\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : 261.56549072265625\n",
            "Train_StdReturn : 36.64844512939453\n",
            "Train_MaxReturn : 320.2608947753906\n",
            "Train_MinReturn : 192.08297729492188\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 20000\n",
            "TimeSinceStart : 436.5903549194336\n",
            "Training Loss : 0.10086335986852646\n",
            "Initial_DataCollection_AverageReturn : -2504.96630859375\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 270.9199523925781\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 270.9199523925781\n",
            "Eval_MinReturn : 270.9199523925781\n",
            "Eval_AverageEpLen : 500.0\n",
            "Train_AverageReturn : 290.55084228515625\n",
            "Train_StdReturn : 32.508949279785156\n",
            "Train_MaxReturn : 338.9970703125\n",
            "Train_MinReturn : 239.37030029296875\n",
            "Train_AverageEpLen : 500.0\n",
            "Train_EnvstepsSoFar : 25000\n",
            "TimeSinceStart : 575.4877843856812\n",
            "Training Loss : 0.1008116602897644\n",
            "Initial_DataCollection_AverageReturn : -2504.96630859375\n",
            "Done logging...\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# python cs285/scripts/run_hw4_mb.py --exp_name q5_cheetah_cem_1000 --env_name \n",
        "# 'cheetah-cs285-v0' --mpc_horizon 15 --add_sl_noise --num_agent_train_steps_per_iter 1500\n",
        "#  --batch_size_initial 5000 --batch_size 5000 --n_iter 5 \n",
        "# --mpc_action_sampling_strategy 'random'\n",
        "# \n",
        "args = Args()\n",
        "args.exp_name = 'q5_cheetah_random'\n",
        "args.env_name = 'cheetah-cs285-v0'\n",
        "args.mpc_horizon = 15\n",
        "args.add_sl_noise = True\n",
        "args.num_agent_train_steps_per_iter = 1500\n",
        "args.batch_size_initial = 5000\n",
        "args.batch_size = 5000\n",
        "args.n_iter = 5\n",
        "args.mpc_action_sampling_strategy = 'random'\n",
        "\n",
        "logdir = 'hw4_' + args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join(data_path, logdir)\n",
        "args['logdir'] = logdir\n",
        "if not(os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)\n",
        "\n",
        "print(\"LOGGING TO: \", logdir)\n",
        "\n",
        "trainer = MB_Trainer(args)\n",
        "trainer.run_training_loop()\n",
        "\n",
        "# python cs285/scripts/run_hw4_mb.py --exp_name q5_cheetah_cem_2 --env_name \n",
        "# 'cheetah-cs285-v0' --mpc_horizon 15 --add_sl_noise --num_agent_train_steps_per_iter 1500\n",
        "#  --batch_size_initial 5000 --batch_size 5000 --n_iter 5 \n",
        "# --mpc_action_sampling_strategy 'cem' --cem_iterations 2\n",
        "\n",
        "args = Args()\n",
        "args.exp_name = 'q5_cheetah_cem_2'\n",
        "args.env_name = 'cheetah-cs285-v0'\n",
        "args.mpc_horizon = 15\n",
        "args.add_sl_noise = True\n",
        "args.num_agent_train_steps_per_iter = 1500\n",
        "args.batch_size_initial = 5000\n",
        "args.batch_size = 5000\n",
        "args.n_iter = 5\n",
        "args.mpc_action_sampling_strategy = 'cem'\n",
        "args.cem_iterations = 2\n",
        "\n",
        "logdir = 'hw4_' + args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join(data_path, logdir)\n",
        "args['logdir'] = logdir\n",
        "if not(os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)\n",
        "\n",
        "print(\"LOGGING TO: \", logdir)\n",
        "\n",
        "trainer = MB_Trainer(args)\n",
        "trainer.run_training_loop()\n",
        "\n",
        "\n",
        "# \n",
        "# python cs285/scripts/run_hw4_mb.py --exp_name q5_cheetah_cem_4 --env_name \n",
        "# 'cheetah-cs285-v0' --mpc_horizon 15 --add_sl_noise --num_agent_train_steps_per_iter 1500\n",
        "#  --batch_size_initial 5000 --batch_size 5000 --n_iter 5 \n",
        "# --mpc_action_sampling_strategy 'cem' --cem_iterations 4\n",
        "\n",
        "args = Args()\n",
        "args.exp_name = 'q5_cheetah_cem_4'\n",
        "args.env_name = 'cheetah-cs285-v0'\n",
        "args.mpc_horizon = 15\n",
        "args.add_sl_noise = True\n",
        "args.num_agent_train_steps_per_iter = 1500\n",
        "args.batch_size_initial = 5000\n",
        "args.batch_size = 5000\n",
        "args.n_iter = 5\n",
        "args.mpc_action_sampling_strategy = 'cem'\n",
        "args.cem_iterations = 4\n",
        "\n",
        "logdir = 'hw4_' + args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join(data_path, logdir)\n",
        "args['logdir'] = logdir\n",
        "if not(os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)\n",
        "\n",
        "print(\"LOGGING TO: \", logdir)\n",
        "\n",
        "trainer = MB_Trainer(args)\n",
        "trainer.run_training_loop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kTH-tXkI-B-"
      },
      "outputs": [],
      "source": [
        "#@markdown You can visualize your runs with tensorboard from within the notebook\n",
        "\n",
        "## requires tensorflow==2.3.0\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ../../data/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "run_hw4_mb.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "6bc88d8eca3416184620d8635cba4a6a799ba1af50e3110c0cd6d1a8e942dd5d"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 64-bit ('cs285': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
